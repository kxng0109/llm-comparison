# Copy this file to .env and fill in your values
# Only configure the providers you want to use
SERVER_PORT=8080

# Add your frontend URL(s) - comma separated for multiple origins
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Get your API key from: https://platform.openai.com/api-keys
# Uncomment and set if you want to use OpenAI models
#SPRING_AI_OPENAI_API_KEY=sk-your-openai-api-key-here
#SPRING_AI_OPENAI_CHAT_MODEL=gpt-4
#SPRING_AI_OPENAI_CHAT_TEMPERATURE=0.7

# Get your API key from: https://console.anthropic.com/
# Uncomment and set if you want to use Anthropic Claude models
#SPRING_AI_ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
#SPRING_AI_ANTHROPIC_CHAT_MODEL=claude-3-opus-20240229
#SPRING_AI_ANTHROPIC_CHAT_TEMPERATURE=0.7

# Ollama runs locally and requires no API key
# Download from: https://ollama.ai/download
# Start Ollama server: ollama serve
# Pull models: ollama pull llama3.2:1b (you can specify an model you want)
# Go to https://ollama.com/search for more models
SPRING_AI_OLLAMA_BASE_URL=http://localhost:11434
SPRING_AI_OLLAMA_CHAT_MODEL=llama3.2:1b
#SPRING_AI_OLLAMA_CHAT_TEMPERATURE=0.7

# ==============================================
# NOTES
# ==============================================
# - You can use any combination of providers
# - At least one provider must be configured
# - Ollama is free and recommended for testing
# - Commercial providers (OpenAI/Anthropic) require payment
# - Never commit your .env file to version control